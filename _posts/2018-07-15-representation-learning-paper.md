---
title: 使用“对比预测编码”的表征学习
date:  2018-07-21
article:
    title: Representation Learning with Contrastive Predictive Coding
    publish_at: 10 Jul 2018
    authors:
      - Aaron van den Oord
      - Yazhe Li
      - Oriol Vinyals
    url: https://arxiv.org/abs/1807.03748
tags: ["论文笔记"]
abstract: >-
  Contrastive Predictive Coding, DeepMind
---

## 摘要

摘要里显示，本工作主要引入了两个创新点：

1. 引入了自回归模型
2. 使用了一个“概率对比损失函数”

新提出的方法说：Contrastive Predictive Coding。猜想这个 Contrastive 就是“概率对比损失函数”，而 Predictive 就是自回归模型。这里还提到以前的工作主要是评估学习到的表征的对某一特点形态的表达。而这个工作则是直接提升了在几个实际领域的学习效果。

## 1. 引言

引言里提到："Predictive Coding" 是数据压缩领域里处理信号的一种非常古老的方法。也就是说这个想法的一个源头是对数据进行压缩。而这个 "Predictive Coding" 又是什么东西？已经有一些基于“Predictive Coding”的无监督学习方法的实践，但作者认为他们都是部分有效的。因为上下文并不是完全独立的。所以本文作者就引入了表征学习来自动学习需要的特征。

本文作者提出的方法分成三步：

1. 把数据压缩到一个更紧密的空间
2. 使用自回归模型对未来的多步进行预测
3. 引入使用“噪声对比估计”损失函数

这里，第二步不是很理解，如果是对图像一类的数据，怎么使用自回归预测未来的多步数据？

## 2. Contrastive Predictive Coding

建模的直观想法就是：对底层共享的信息进行编码，去掉噪声和低级别的信息。当进行单步预测的时候，通常利用的只是信号的平滑性，也就是说在间隔时间很小的时候，信号不会发生突变。但当预测的深度增加以后，这些预测之间的共享信息就会减少，所以模型就不得不使用一些有全局性的特征结构。而这些结构可能更具有普遍性。作者举了几个例子，比如“音素”、“对象”等。

这里提到，在预测高维数据的时候，单峰的损失函数不起作用。这里并没有提到原因。但在我的一次实验中，确实是这样的。虽然我通过 MSE 想要达到生成的图和原图一致，但中间结果确完全没有想要的结果。

看 2.2 节的意思，所谓 “Contrastive Predictive Coding”，应该就是前面一个卷积网络做降维，然后接一个 LSTM/GRU 做自回归。然后文章也提到一些新近的自回归模型。

公式的排版乱了，看不出来要怎么实现 loss 函数。

## 3. 实验

3.1 的音频实验直接略过，因为也不太懂语音识别。在 3.2 节看对于于图像的处理过程，大概是这样的：先把图像切成一个系列的小图，通过一个 ResNet 做 encoder，把每个小图最后变成一个一维的向量，然后再通过一个 PixelCNN 型的网络做自回归预测。因为前面已经切成小图，而且每个图都有一些重叠，所以就通过这些小图的序列当作类似时间序列的数据进行预测。

## 结论

因为中间有个自回归模型，所以这个东西需要很大的计算量。不过它提供了两个重要的思路：

1. 通过把输入图像切片，可以形成一个具有相关性的序列，从而可以使用 RNN 型的模型进行建模
2. 使用了一个新的损失函数，可以让模型进行 end-to-end 的训练
